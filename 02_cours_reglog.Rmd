# Estimation des paramètres


On considère dans ce paragraphe la regression logistique multivariée.

Comme en régression linéaire, l’objet de cette modélisation est *d’estimer les coefficients $\beta_0$ et $\beta_1$*



## Vraisemblance

**Définition** : Probabilité d’observer les évènements $y_i$ en considérant le modèle $\mathcal{M}$

**Important** : la vraisemblance dépend

  - des observations ($y_i$)
  - du modèle $\mathcal{M}$

**Notations** : 
 
- les covariables globales théoriques $X=(X_1,...,X_p)$
- les covariables globales observées $x=(x_1,...,x_p)$. 
- les covariables individuelles observées $x_i=(x_{1i},...,x_{pi})$

Le modèle conditionnel de $Y$ sachant $X$ suit une loi de Bernoulli de paramètres $\pi(x)$ : 

$$E(Y|X = x) \sim \mathcal B (\pi(x))$$


## Contribution des observations

Pour l’observation $i$, la contribution à la vraisemblance est donc (Bernoulli) : 

$$l(x_i, y_i) = P(y_i = 1|x_i)^{y_i} (1-P(y_i=1|x_i))^{1-y_i} = \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} $$

si $y_i = 1$ : 
\begin{eqnarray}
l(x_i, 1) &=& P(y_i = 1|x_i)^1 (1-P(y_i=1|x_i))^0 &=& \pi(x_i)^1 (1-\pi(x_i))^0 \\
&=& P(y_i = 1|x_i) 1 &=& \pi(x_i) 1 \\
&=& P(y_i = 1|x_i) &=& \pi(x_i) \\
\end{eqnarray}

si $y_i = 0$ : 
\begin{eqnarray}
l(x_i, 0) &=& P(y_i = 1|x_i)^0 (1-P(y_i=1|x_i))^1 &=& \pi(x_i)^0(1-\pi(x_i))^1 \\
&=& 1 (1-P(y_i=1|x_i)) &=& 1 (1-\pi(x_i)) \\ 
&=& 1-P(y_i=1|x_i) &=& 1-\pi(x_i) \\ 
\end{eqnarray}


```{r}
py1x = function(t,m) {
  x = m$coefficients[[1]] + m$coefficients[[2]]*t
  1/(1 + exp(-x))
}
d[c(2, 16),c("Bwt", "Sex")]
layout(matrix(1:2, 1), respect=TRUE)
plot(d$Bwt, s, main="Sex~Bwt", xlab="Bwt", ylab="Sex")
# \mathbb{P}(Y=1|X) = logitinv(a + b.x)
m = glm(d$Sex~d$Bwt, family = binomial(logit))
m$coefficients
logitinv = function(x) 1/(1 + exp(-x))
x = seq(min(d$Bwt), max(d$Bwt), length.out=30)
lines(x, logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x), col=1, lwd=2)
legend("bottomright", "Y=Pi(X)", col=1, lty=1, cex=0.6)
legend("bottomright", c("Y=Pi(X)", "1 - l(xi, yi)"), col=1:2, lty=1, cex=0.6)

points(d$Bwt[2], s[2], col=2, pch=16)
arrows(d$Bwt[2], 0, d$Bwt[2], py1x(d$Bwt,m)[2], col=adjustcolor(2, alpha.f=0.5), length=0.05, lwd=3)

points(d$Bwt[110], s[110], col=2, pch=16)
arrows(d$Bwt[110], 1, d$Bwt[110], py1x(d$Bwt,m)[110], col=adjustcolor(2, alpha.f=0.5), length=0.05, lwd=3)

plot(d$Bwt, s, main="Sex~Bwt", xlab="Bwt", ylab="Sex")
arrows(d$Bwt, s, d$Bwt, py1x(d$Bwt,m), col=adjustcolor(4, alpha.f=0.2), length=0.05, lwd=3)
legend("bottomright","1 - P(Y=yi|X=xi)", col=4, lty=1, cex=0.6)
```







## Maximisation de la vraisemblance

Le modèle s’écrit:  $logit(E(Y|X_1,...,X_p)) = \beta_0 + \beta_1X_1+...+\beta_pX_p$ avec $\beta = (\beta_0,\beta_1, ..., \beta_p)$ inconnus.

On estime $\beta$ par **maximum de vraisemblance**.

La vraisemblance conditionnelle est :

$$\mathcal{L}_n(\beta) = \prod_{i=1}^{n} l(x_i, y_i) = \prod_{i=1}^{n} \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} = \prod_{i=1}^{n} \Big(\frac{\pi(x_i)}{1-\pi(x_i)}\Big)^{y_i}(1-\pi(x_i))$$


En passant au logarithme, on a :

$$log(\mathcal{L}_n(\beta)) = \sum_{i=1}^{n}\Big[ y_i * log \Big(\frac{\pi(x_i)}{1-\pi(x_i)}\Big) + log(1-\pi(x_i))\Big] $$

Cette fonction contient des valeurs observées $y_i$ et des valeurs prédites $\pi(x_i)$ qui dépendent de $\beta$

On maximise cette fonction en trouvant la valeur de $\beta$ *pour laquelle la dérivée (par rapport à $\beta$) est nulle* ($\mathcal{L}_n’(\beta) = 0$), definissant ainsi : 

$$\widehat{\beta}_n = argmax( \mathcal{L}_n(\beta) )$$

**Remarques**  :

Il n’existe pas de **solution analytique** de $\mathcal{L}_n’(\beta) = 0$
L’estimateur $\widehat{\beta}$ est obtenus **par approximation**, les résultats peuvent différer en fonction de l’algorithme utilisé. 
Exemple d’algorithme simple : 

1) initialisation des valeurs de $\beta$ de manière aléatoire et calcule la vraisemblance associée
2) modification de la valeur de $\beta$, est-ce que cette nouvelle valeur améliorent la vraisemblance ? 
3) si cette nouvelle valeur améliore la vraisemblance on la conserve sinon on la rejette
4) itération jusqu’à convergeance (la vraisemblance devient stable, critère d’arrêt)


```{r}
layout(matrix(1:2, 1), respect=TRUE)
plot(d$Bwt, s, main="Sex~Bwt, coef[1] step by 1", xlab="Bwt", ylab="Sex")
# \mathbb{P}(Y=1|X) = logitinv(a + b.x)
x = seq(min(d$Bwt), max(d$Bwt), length.out=30)
m = glm(d$Sex~d$Bwt, family = binomial(logit))
m$coefficients
coefs = m$coefficients
lines(x, logitinv(coefs[[1]] + coefs[[2]]*x), col=1, lwd=2)
coefs[1] = coefs[1] - 5 
for (i in 1:10) {
  lines(x, logitinv(coefs[[1]] + coefs[[2]]*x), col=1, lwd=2, lty=2)
  coefs[1] = coefs[1] + 1
}


plot(d$Bwt, s, main="Sex~Bwt, coef[2] step by 0.01", xlab="Bwt", ylab="Sex")
# \mathbb{P}(Y=1|X) = logitinv(a + b.x)
x = seq(min(d$Bwt), max(d$Bwt), length.out=30)
m = glm(d$Sex~d$Bwt, family = binomial(logit))
m$coefficients
coefs = m$coefficients
lines(x, logitinv(coefs[[1]] + coefs[[2]]*x), col=1, lwd=2)
coefs[2] = coefs[2] - 1 
for (i in 1:10) {
  lines(x, logitinv(coefs[[1]] + coefs[[2]]*x), col=1, lwd=2, lty=2)
  coefs[2] = coefs[2] + 0.2  
}
```


## Propriétés de l’estimateur

Asymptotiquement (i.e quand n tend vers l’infini), l’estimateur du maximum de vraisemblance:

1) existe et est unique
2) est sans biais  (i.e. il tend vers sa valeur réelle)
3) est de distribution normale
4) est efficace (i.e. de variance minimale parmi tous les estimateurs sans biais obtenus avec d’autres méthodes).

- Ainsi: $\lim_{n\to\infty} \sqrt{n}(\widehat{\beta}-\beta) \rightarrow N(0,\Sigma^{-1})$ avec $\Sigma^{-1}$ la matrice de variance-covariance de $\beta$ 


- On en déduit les **intervalles de confiance** pour $\beta_k$:

$$ IC(\widehat{\beta}) = \Big[\widehat{\beta_k} -t_{1-\alpha/2,n-2} * \sqrt{\widehat{var}(\widehat{\beta_k})} ; \widehat{\beta_k}+t_{1-\alpha/2,n-2} * \sqrt{\widehat{var}(\widehat{\beta_k})} \Big]$$

Avec  $t_{1-\alpha/2,n-2}$ le quantile de niveau $1-\alpha/2$ de la loi de student à (n-2) degrés de liberté 

En pratique approximé dès que $n>30$ par $u_{1-\alpha/2}$ le quantile de niveau $1-\alpha/2$ de la loi normale









