# Le modèle logistique

On cherche une relation entre une **variable binaire** $Y$ (observée, à expliquer) et un ensemble de variables explicatives $X$ (quantitatives ou qualitatives). 

$$Y \sim X$$


On distingue : 

  - les modèles explicatifs
  - les modèles predictifs

Exemples de variables binaires à expliquer : 
  
  - Le sexe en fonction du poids (regression logistique simple)
  - Maladie coronarienne en fonction d’HTA et cholestérol
  - Survenue de cancer en fonction d’expositions chimiques
  - Consommation d’un bien selon variables sociodémographiques
  - Risque d’accoucher d’un bébé de faible poids (<2500g) en fonction de l’âge de la mère, du poids, du tabagisme.
  - le sexe en fonction de l’expression des gènes (challenge *sexpred*)
  - l’histologie en fonction de l’expression des gènes (challenge *histpred*)
  - la survie à 24 mois en fonction de l’expression des gènes (challenge *virpred*)




## Construction du modèle logistique

On considère dans ce paragraphe la regression logistique univariée : $Y \sim x$, avec x une variable quantitaive.

On pose $Y$ la variable binaire, $x$ le vecteur de la variable explicative et **on veut modéliser : ** 

$$E(Y|x) = \mathbb{P}(Y=1|x) = \pi (x)$$


### Variable de Bernoulli

Toute variable binaire peut être codée par 0 ou 1. 
On considére alors cette variable comme une variable de Bernoulli qui suit la loi de probabilité de paramètre $p$ :
 
$$\mathbb{P}(Y=1)=p$$
$$\mathbb{P}(Y=0)=1-p$$ 

résumé par :

$$\mathbb{P}(Y=k)=p^k(1-p)^{1-k}, k \in \{0,1\}$$ 

**Note** : si on considére $p$ constant (indépendant de $x$), la valeur de $p$ qui maximise la vraisemblance des observations est la proportion de 1 dans les observations (*exercice : proposer une démonstration*). 

### Approche naïve avec la regression lineaire

Considérons que Y suit une loi de Bernoulli de paramètre $\pi (x)$ une fonction linéaire de $x$ : 

$$\mathbb{P}(Y=1|x)=\pi (x) = \beta_0 + \beta_1 x$$

Problème $\pi(x)$ prends des valeurs négatives et des valeurs supérieur à 1

```{r echo=TRUE}
d = MASS::cats
```

```{r}
layout(1, respect=TRUE)
s = as.numeric(d$Sex) - 1
plot(d$Bwt, s, main="Regression linéaire (Sex~Bwt)", xlab="Bwt", ylab="Sex", ylim=c(0,1.5))
m = lm(s~d$Bwt)
abline(m, col=2, lwd=2)
# abline(h=0.5, col="grey", lwd=2)
arrows(d$Bwt, s, d$Bwt, s-m$residuals, col=adjustcolor(4, alpha.f=0.2), length=0.1, lwd=2)
legend("bottomright",c("regression line", "residuals"), col=c(2,4), lty=1, cex=0.6)
```


### la fonction logit

\begin{eqnarray}
              \text{logit: } ]0,1[ & \rightarrow & \mathbb{R} \\
                                 x & \rightarrow & logit(x)  =  log(\frac{x}{1-x}) \\ 
\end{eqnarray}

\begin{eqnarray}
              \text{logit$^{-1}$: } \mathbb{R}& \rightarrow &  ]0,1[  \\
                      x & \rightarrow & logit^{-1}(x) = \frac{1}{1+e^{-x}}  
\end{eqnarray}

```{r}
layout(matrix(1:2, 1), respect=TRUE)
x = 0:100/100
plot(x, log(x/(1-x)), main="logit", type="l")
x = seq(-4, 4,  length.out=100)
plot(x, 1 / (1+exp(-x)), main="logit^-1", type="l")
```

$$ \lim_{x\to0} logit(x) = -\infty \ \ \ \ \ \ \ \ \ \  \lim_{x\to1} logit(x) = +\infty $$

$$ \lim_{x\to-\infty} logit^{-1}(x) = 0 \ \ \ \ \ \ \ \ \ \  \lim_{x\to+\infty} logit^{-1}(x) = 1 $$






### Présentation du modèle logistique simple

Considérons que Y suit une loi de Bernoulli de paramètre $\pi (x)$ telle que :

$$\mathbb{P}(Y=1|x) = \pi(x) = logit^{-1}(\beta_0 + \beta_1 x)$$ 

On appelle $\pi(x)$, le prédicteur de Y en fonction de x.

```{r}
layout(1, respect=TRUE)
plot(d$Bwt, s, main="Sex~Bwt", xlab="Bwt", ylab="Sex")
m = glm(d$Sex~d$Bwt, family = binomial(logit))
m$coefficients
logitinv = function(x) 1/(1 + exp(-x))
x = seq(min(d$Bwt), max(d$Bwt), length.out=30)
lines(x, logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x), col=2, lwd=2)
py1x = function(t,m) {
  x = m$coefficients[[1]] + m$coefficients[[2]]*t
  1/(1 + exp(-x))
}
arrows(d$Bwt, s, d$Bwt, py1x(d$Bwt,m), col=adjustcolor(4, alpha.f=0.2), length=0.05, lwd=3)
legend("bottomright", c("Y=Pi(X)=logit^-1(b.X)", "1 - P(Y=y_i|X=x_i)"), col=c(2,4), lty=1, cex=0.6)
```

On **généralise** aisement le modèle logistique pour des variables explicatives **multivariées** $X=(X_1,...,X_p))$, $X_i$ pouvant être **qualitatives** ou **quantitaves**, $\beta=(\beta_0, \beta_1, ..., \beta_p)$ : 


$$\mathbb{P}(Y=1|X) = \pi(X) = logit^{-1}(\beta X)$$ 














## Mesures d'interêt

On définit les **Odds** :

$$Odds(X) = \frac{\pi(X)}{1-\pi(X)}$$

Ce qui correspond à la cote d’un événement, la probabilité que l’événement se produise par rapport à la probabilité qu’il ne se produise pas. 

Ainsi : 

\begin{eqnarray}
\mathbb{P}(Y=1|X) &=& \pi(X) &=& logit^{-1}(\beta X) \\
logit(\mathbb{P}(Y=1|X)) &=& logit(\pi(X)) &=& \beta X \\
&&log(\frac{\pi(x)}{1-\pi(x)}) &=& \beta X \\
&&log(Odds(X)) &=& \beta X \\
&&Odds(X) &=& e^{\beta X}

\end{eqnarray}

Notons au passage que $logit(\mathbb{P}(Y=1|X)) = log(Odds(X))$ est une varible aléatoire modélisée par un modèle linéaire. 
C’est à partir de ce modèle linéaire que l’on obtient les p-valeurs du modèle logistique. La regression logistique est un modéle linéaire géneralisé (*glm*) qui utilise la fonction *logit* comme fontion de lien.


```{r echo=TRUE, results="verbatim"}
m = glm(d$Sex~d$Bwt, family = binomial(logit))
m$coefficients
summary(m)
```

On définit définit les **Odds Ratio** :

$$OR_{u/v} = \frac{odd(X = u)}{odd(X=v)} = e^{\beta (u-v)}$$

$OR$ estime par exemple le rapport malades/non-malades entre deux populations (facilement compréhensible quand $X$ est qualitatif).

$OR$ est directement calculable à partir des coefficients de la régression $\beta$, il permet d’interpréter les $\widehat{\beta_k}$ et de mesurer l‘effet de la variable $X_k$ sur le modèle.





On définit définit le **risque relatif** :

$$ RR_{u/v} = \frac{\pi(X = u)}{\pi(X=v)} = \frac{P(Y=1|X=u)}{P(Y=1|X=v)} $$

Note : si $p$ est petit alors $\frac{p}{1-p} \simeq p$ et l’Odds Ratio est proche du risque relatif.


$RR$ estime le risque (*i.e.* la probabilité) d’être par exemple malade.

$OR$ et $RR$ donnent la même indication sur la relation entre $Y$ et $X$ :

1) Si $RR_{u/v}$ (ou $OR_{u/v}$) $>1$ alors il y a plus de risque de $Y=1$ si $X=u$ que si $X=v$
2) Si $RR_{u/v}$ (ou $OR_{u/v}$) $<1$ alors il y a moins de risque de $Y=1$ si $X=u$ que si $X=v$
3) Si $RR_{u/v}$ (ou $OR_{u/v}$) $=1$ alors $Y$ n’est pas influencée par $X=u$ vs. $X=v$ (i.e. Y indépendant des catégories $u$ et $v$ de $X$)


















